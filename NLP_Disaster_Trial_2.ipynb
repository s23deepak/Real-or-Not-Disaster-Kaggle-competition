{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport re\nimport os\nimport string\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.initializers import Constant\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nlemmatizer = WordNetLemmatizer()\nfrom sklearn.model_selection import train_test_split","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/glove6b100d-2/glove.6B.100d.txt\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_csv = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsample_submission_csv = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\ntrain_csv.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"I think this Location column is not going to help much for model accuracy in both training or prediction.<br>\nHence i am going to drop that column."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_csv\ntrain_df = train_df.drop('location', axis = 1)\ntrain_df.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   id keyword                                               text  target\n0   1     NaN  Our Deeds are the Reason of this #earthquake M...       1\n1   4     NaN             Forest fire near La Ronge Sask. Canada       1\n2   5     NaN  All residents asked to 'shelter in place' are ...       1\n3   6     NaN  13,000 people receive #wildfires evacuation or...       1\n4   7     NaN  Just got sent this photo from Ruby #Alaska as ...       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now starts helper functions for data preprocessing<br>\nI have focused on the following:\n1. Removing Web links\n2. Removing @'s\n3. Removing Hashtags\n4. Rewrite words with single qoutes\n5. Removing punctuations\n6. Removing Stop Words\n7. Applying Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#re.findall(r'[\\w\\.-]+@[\\w\\.-]+', tweet) ----> email\ndef remove_hashtag(tweet) :\n    hash_part = re.findall(r'#(\\w+)',tweet)\n    for index,words in enumerate(hash_part) :\n        sub = re.findall(r'[A-Z][A-Z0-9]+', words)\n        if sub :\n            split_word = words.split(sub[0])\n            string = sub[0] + \" \" + split_word[1]\n            if split_word[1] :\n                tweet = re.sub(r'#'+sub[0]+'[a-z]+',string,tweet)\n            else :\n                tweet = re.sub(r'#'+sub[0],string,tweet)\n        elif re.findall(r'[A-Z][a-z]+',words) :\n            sub = re.findall(r'[A-Z][a-z]+',words)\n            string = \"\"\n            if len(sub) !=1 :\n                for sub_word in sub :\n                    string = string + \" \"  + sub_word\n                tweet = re.sub(r'#'+sub[0]+'[a-zA-Z]+', string, tweet)\n            else :\n                string = string + sub[0] + \" \"\n                tweet = re.sub(r'#'+sub[0], string, tweet)\n        else :\n            tweet = re.sub(r'#'+hash_part[index], hash_part[index], tweet)\n    return tweet\n\ndef rewrite_single_qoute(tweet) :\n    tweet = re.sub(r'i\\'m', 'i am', tweet)\n    tweet = re.sub(r'I\\'m', 'i am', tweet)\n    tweet = re.sub(r'can\\'t', 'can not', tweet)\n    tweet = re.sub(r'omg|Omg', 'oh my god', tweet)\n    #tweet = re.sub(r'', '', tweet)\n    tweet = re.sub(r'(\\w+)\\'re', '\\g<1>are', tweet)\n    tweet = re.sub(r'(\\w+)\\'s', '\\g<1>is', tweet)\n    tweet = re.sub(r'dammit', 'damn it', tweet)\n    tweet = re.sub(r'won\\'t', 'will not', tweet)\n    tweet = re.sub(r'wont', 'will not', tweet)\n    tweet = re.sub(r'(\\w+)\\'d', '\\g<1>would', tweet)\n    return tweet\n\ndef remove_additional(tweet) :\n    tweet = re.sub('[^a-zA-z]+',' ',tweet)\n    return tweet\n\n\ndef data_preprocessing(tweet) :\n    tweet = re.sub(r'http\\S+', '', tweet)\n    tweet = re.sub(r'@(\\w*)', '', tweet)\n    tweet = remove_hashtag(tweet)\n    tweet = re.sub(r'\\n',' ', tweet) \n    tweet = re.sub('\\s+', ' ', tweet).strip()\n    # rewriting words containing single qoute \n    tweet = rewrite_single_qoute(tweet)\n    tweet = remove_additional(tweet)#removing additional unwanted characters (and also remaining Hashtags)\n    tweet = tweet.lower()\n    #stop words code to be written below\n    tweet = tweet.split()\n    tweet = [w for w in tweet if not w in set(stopwords.words('english'))] \n    tweet = ' '.join(tweet)\n    #lemmatization\n    token_list = word_tokenize(tweet)\n    tweet = ' '.join([lemmatizer.lemmatize(w) for w in token_list])\n    return tweet","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling null values in keyword with an empty string and appending to their respective texts(tweets)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"keyword\"] = train_df[\"keyword\"].fillna(' ')\ntrain_df['joined'] = train_df[['keyword', 'text']].apply(lambda x: ' '.join(x), axis = 1)\ntrain_df.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"   id keyword                                               text  target  \\\n0   1          Our Deeds are the Reason of this #earthquake M...       1   \n1   4                     Forest fire near La Ronge Sask. Canada       1   \n2   5          All residents asked to 'shelter in place' are ...       1   \n3   6          13,000 people receive #wildfires evacuation or...       1   \n4   7          Just got sent this photo from Ruby #Alaska as ...       1   \n\n                                              joined  \n0    Our Deeds are the Reason of this #earthquake...  \n1             Forest fire near La Ronge Sask. Canada  \n2    All residents asked to 'shelter in place' ar...  \n3    13,000 people receive #wildfires evacuation ...  \n4    Just got sent this photo from Ruby #Alaska a...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>text</th>\n      <th>target</th>\n      <th>joined</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td></td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td></td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td></td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>All residents asked to 'shelter in place' ar...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td></td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>13,000 people receive #wildfires evacuation ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td></td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>Just got sent this photo from Ruby #Alaska a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing is then applied to the joined column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['joined'] = train_df['joined'].apply(lambda x : data_preprocessing(x))\ntrain_df.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"   id keyword                                               text  target  \\\n0   1          Our Deeds are the Reason of this #earthquake M...       1   \n1   4                     Forest fire near La Ronge Sask. Canada       1   \n2   5          All residents asked to 'shelter in place' are ...       1   \n3   6          13,000 people receive #wildfires evacuation or...       1   \n4   7          Just got sent this photo from Ruby #Alaska as ...       1   \n\n                                              joined  \n0         deed reason earthquake may allah forgive u  \n1              forest fire near la ronge sask canada  \n2  resident asked shelter place notified officer ...  \n3  people receive wildfire evacuation order calif...  \n4  got sent photo ruby alaska smoke wildfire pour...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>text</th>\n      <th>target</th>\n      <th>joined</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td></td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>deed reason earthquake may allah forgive u</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td></td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>forest fire near la ronge sask canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td></td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>resident asked shelter place notified officer ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td></td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>people receive wildfire evacuation order calif...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td></td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Here is the set of stop words that can be removed from the tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nprint(stop_words)","execution_count":8,"outputs":[{"output_type":"stream","text":"{'you', 'we', 'because', \"you'd\", 'them', 'he', 'mustn', 'below', 'does', 'into', 'the', 'above', \"hadn't\", 'do', \"weren't\", 'if', 'up', 'most', 'having', 'her', 'until', 'weren', 's', 'where', 'him', 'hers', \"mustn't\", \"needn't\", 'once', 'me', 'being', \"should've\", 'wasn', \"that'll\", 'for', 'yourself', 'own', \"wasn't\", 'am', 'before', 'm', 'again', 'same', 'be', 'aren', \"haven't\", 'whom', 'as', 'a', 'why', 'now', 'between', 'than', 'then', \"aren't\", \"couldn't\", 'an', 'hadn', 've', \"isn't\", 'nor', 'has', 'under', 'needn', 'to', 'on', 'here', 'in', 'some', 'shan', 'ourselves', 'against', 'don', 'themselves', \"didn't\", 'shouldn', 'out', 'have', 're', 'haven', 'so', 'my', 'didn', 'down', 'too', 'few', 'been', 'this', 'o', 'his', \"won't\", 'each', 'they', \"hasn't\", 'are', 'were', 'ain', 'no', 'himself', 'all', 'of', \"she's\", 'is', 'during', 'couldn', \"mightn't\", 'very', \"you're\", 'such', 'both', 'which', 'ours', 'did', 'by', 'these', 'but', \"don't\", 'y', 'our', 'theirs', \"shouldn't\", \"wouldn't\", 'was', 'after', 'further', 'from', 'only', 'yourselves', 'how', 'won', 'any', 'yours', 'myself', 'through', 'its', 'while', 'can', 'at', \"doesn't\", 'it', 'doesn', 'your', 'and', 'she', \"you'll\", 'herself', 'over', 'will', 'what', 'ma', 'doing', 'wouldn', 'those', 't', 'that', 'd', 'mightn', 'other', 'about', 'or', 'not', 'had', \"it's\", 'just', 'who', 'should', 'isn', 'off', 'with', 'their', 'there', 'll', 'more', 'i', 'when', \"you've\", 'hasn', 'itself', \"shan't\"}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Finding the length of longest tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_max_length = max(len(x) for x in train_df[\"joined\"])\nprint(tweet_max_length)","execution_count":9,"outputs":[{"output_type":"stream","text":"156\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Creating a corpus of words from tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df):\n    corpus=[]\n    for tweet in df['joined'] :\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus  \n\ncorpus=create_corpus(train_df)","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating an Embedding dictionary, from Standford's glove represtation.<br> I chose glove represtation with 100 dimensions."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open('/kaggle/input/glove6b100d-2/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]  # added\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenization is done on courpus and then tweets are tokenized.<br>\nWith the Embedding dictionary created and word_index obtained from tokenizer an Embedding Matrix is created.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n\nword_index=tokenizer.word_index\n\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in word_index.items() :\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec  \n\n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#HYPER PARAMETERS\nmax_length = tweet_max_length\npadding_type = 'post'\ntrunc_type = 'post'\noov_tok = '<OOV>'","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_tweets, validation_tweets, training_targets, validation_targets = train_test_split(train_df['joined'], \n                                                                                            train_df['target'], \n                                                                                            test_size=0.2, \n                                                                                            random_state=23)\n\n#Sequencing and padding training data\ntraining_sequences = tokenizer.texts_to_sequences(training_tweets)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n#Sequencing and padding validation data\nvalidation_sequences = tokenizer.texts_to_sequences(validation_tweets)\nvalidation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix), input_length=max_length,trainable=False),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units = 64, dropout=0.2, recurrent_dropout = 0.2, return_sequences = True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units = 32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\noptimzer=Adam(lr=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel.summary()\n\nhistory=model.fit(training_padded,training_targets,batch_size=4,epochs=10,validation_data=(validation_padded,validation_targets),verbose=1)","execution_count":null,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 156, 100)          1272600   \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 156, 100)          0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 156, 128)          63744     \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 64)                31104     \n_________________________________________________________________\ndense (Dense)                (None, 64)                4160      \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 1,371,673\nTrainable params: 99,073\nNon-trainable params: 1,272,600\n_________________________________________________________________\nTrain on 6090 samples, validate on 1523 samples\nEpoch 1/10\n6090/6090 [==============================] - 549s 90ms/sample - loss: 0.5111 - accuracy: 0.7552 - val_loss: 0.4476 - val_accuracy: 0.8050\nEpoch 2/10\n6090/6090 [==============================] - 529s 87ms/sample - loss: 0.4619 - accuracy: 0.7956 - val_loss: 0.4308 - val_accuracy: 0.8056\nEpoch 3/10\n6090/6090 [==============================] - 525s 86ms/sample - loss: 0.4442 - accuracy: 0.8018 - val_loss: 0.4152 - val_accuracy: 0.8168\nEpoch 4/10\n6090/6090 [==============================] - 524s 86ms/sample - loss: 0.4205 - accuracy: 0.8156 - val_loss: 0.4416 - val_accuracy: 0.8109\nEpoch 5/10\n6090/6090 [==============================] - 530s 87ms/sample - loss: 0.4094 - accuracy: 0.8227 - val_loss: 0.4093 - val_accuracy: 0.8267\nEpoch 6/10\n6090/6090 [==============================] - 530s 87ms/sample - loss: 0.3974 - accuracy: 0.8314 - val_loss: 0.4086 - val_accuracy: 0.8267\nEpoch 7/10\n6090/6090 [==============================] - 531s 87ms/sample - loss: 0.3709 - accuracy: 0.8414 - val_loss: 0.4018 - val_accuracy: 0.8221\nEpoch 8/10\n6090/6090 [==============================] - 532s 87ms/sample - loss: 0.3643 - accuracy: 0.8433 - val_loss: 0.4055 - val_accuracy: 0.8155\nEpoch 9/10\n2372/6090 [==========>...................] - ETA: 5:02 - loss: 0.3422 - accuracy: 0.8516","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.legend(['accuracy', 'val_accuracy'])\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.legend(['loss', 'val_loss'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv[\"keyword\"] = test_csv[\"keyword\"].fillna(' ')\ntest_csv['joined'] = test_csv[['keyword', 'text']].apply(lambda x: ' '.join(x), axis = 1)\ntest_csv['joined'] = test_csv['joined'].apply(lambda x : data_preprocessing(x))\ntest_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(test_csv['joined'])\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_csv['target'] = (predictions > 0.5).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_csv.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/rtatman/download-a-csv-file-from-a-kernel\n\nfrom IPython.display import HTML\nimport base64\n\ndef create_download_link(df, title = \"Download CSV file\", filename = \"subm.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n\ncreate_download_link(sample_submission_csv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}